{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy # lets up copy things\n",
    "import seaborn as sns #seaborn is a wrapper for matplotlib\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - SIMPLE LINEAR REGRESSION | SUPERVISED LEARNING\n",
    "The main idea of linear regression is to fit a line (in 2D space), a plane (in 3D space), or a hyperplane (in higher dimensions) that best describes the relationship between the independent (input) and dependent (output) variables. This is accomplished by minimizing the sum of squares of the vertical deviations from each data point to the line (called residuals).\n",
    "y = b0 + b1*x + e\n",
    "\n",
    "where:\n",
    "\n",
    "y is the dependent variable,\n",
    "x is the independent variable,\n",
    "b0 is the y-intercept,\n",
    "b1 is the slope of the line (indicating the effect x has on y), and\n",
    "e is the error term.\n",
    "\n",
    "- trying to decrease the residual, \"the line of error\" for the linear prediction compared to the actual data plot\n",
    "\n",
    "Training a linear regression model involves finding the values for the coefficients that minimize the sum of the squared residuals (also known as the loss function), usually using methods such as Gradient Descent or the Normal Equation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to evaluate a linear regression model\n",
    "R-Squared (Coefficient of Determination): This measures the proportion of variance in the dependent variable that can be predicted from the independent variable(s). It takes a value between 0 and 1, where a higher value generally indicates a better fit of the model. However, R-Squared doesn't tell the whole story, as it tends to increase with the addition of more predictors, regardless of whether they are truly meaningful.\n",
    "\n",
    "Adjusted R-Squared: This is a modification of R-Squared that adjusts for the number of predictors in the model. Unlike R-Squared, the adjusted R-Squared increases only if the new term improves the model more than would be expected by chance.\n",
    "\n",
    "Mean Squared Error (MSE) or Root Mean Squared Error (RMSE): These metrics represent the average squared difference or square root of that difference respectively between the observed and predicted values. It's a measure of the model's accuracy and lower values are desirable.\n",
    "\n",
    "Mean Absolute Error (MAE): It represents the average of the absolute differences between the observed and predicted values. It's less sensitive to outliers compared to MSE or RMSE.\n",
    "\n",
    "Residual Plots: Residuals are the difference between the observed and predicted values. Plotting these can give you an idea about the variance and whether itâ€™s constant or not (homoscedasticity).\n",
    "\n",
    "F-statistic: It is a statistical test used to compare our model with a reduced model (no predictors, just intercept). The null hypothesis states that the reduced model is better, and if the F-statistic is sufficiently large, this is evidence against the null hypothesis.\n",
    "\n",
    "T-statistic/P-values: These are used for hypothesis testing on the coefficients. The null hypothesis states that the true coefficient is zero, i.e., the predictor has no effect on the outcome variable. If the p-value associated with a predictor is low (typically < 0.05), we reject the null hypothesis and say that the predictor is statistically significant.\n",
    "\n",
    "Variance Inflation Factor (VIF): VIF measures the correlation and collinearity between the predictor variables in the model. If the VIF is high for a predictor variable (>5 or >10), it means that variable is highly correlated with the other predictor variables, and it might be more challenging to interpret its impact on the output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cols = [\"Date\", \n",
    "                \"bike-count\", \n",
    "                \"Hour\", \n",
    "                \"Temperature\", \n",
    "                \"Humidity\", \n",
    "                \"Windspeed\", \n",
    "                \"Visibility\",\n",
    "                \"Dew\",\n",
    "                \"Solar-radiation\",\n",
    "                \"Rainfall-mm\",\n",
    "                \"Snowfall-cm\",\n",
    "                \"Seasons\",\n",
    "                \"Holiday\",\n",
    "                \"Functional-Day\"]\n",
    "df = pd.read_cvs(\"SeoulBikeData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
